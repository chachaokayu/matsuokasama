{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基本ライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "plt.rcParams['font.family'] = 'Meiryo'\n",
    "from typing import Dict\n",
    "\n",
    "# 深層学習系\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from transformers import EvalPrediction\n",
    "from transformers import default_data_collator\n",
    "from transformers import pipeline\n",
    "\n",
    "import shap\n",
    "\n",
    "# GPUデバイスの仕様有無を確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ整形\n",
    "- 学習データを必要な列のみ取り出し学習・検証に分割する\n",
    "- transformersで読み込める形式へ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "excel_data_path = \"./train_data.xlsx\"\n",
    "origin_df = pd.read_excel(excel_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データの確認（コメントを外して実行）\n",
    "# origin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVE_ID                          0\n",
      "CVE登録機関                         0\n",
      "CWE_ID                          4\n",
      "公開日                             0\n",
      "最終更新日                           0\n",
      "description                     0\n",
      "cvssV3_baseScore                0\n",
      "cvssV3_baseSeverity             0\n",
      "cvssV3_attackVector             0\n",
      "cvssV3_attackComplexity         0\n",
      "cvssV3_privilegesRequired       0\n",
      "cvssV3_userInteraction          0\n",
      "cvssV3_scope                    0\n",
      "cvssV3_confidentialityImpact    0\n",
      "cvssV3_integrityImpact          0\n",
      "cvssV3_availabilityImpact       0\n",
      "cvssV3_exploitabilityScore      0\n",
      "cvssV3_impactScore              0\n",
      "cvssV2_baseScore                0\n",
      "cvssV2_accessVector             0\n",
      "cvssV2_accessComplexity         0\n",
      "cvssV2_authentication           0\n",
      "cvssV2_confidentialityImpact    0\n",
      "cvssV2_integrityImpact          0\n",
      "cvssV2_availabilityImpact       0\n",
      "A+B                             0\n",
      "A                               0\n",
      "B                               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 欠損値の確認\n",
    "print(origin_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 欠損値を埋める\n",
    "origin_df = origin_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 文章と予測したいカラム名を指定\n",
    "text_column = \"description\"\n",
    "label_column = \"A+B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データセット用のデータフレームを作成\n",
    "dataset_df = origin_df[[text_column, label_column]].rename(columns={text_column:'text', label_column:'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 学習、検証用のデータフレームを作成\n",
    "train_df, test_df = train_test_split(dataset_df, test_size=0.2, stratify = dataset_df[\"label\"])\n",
    "\n",
    "# pandasからtransformersで読み込める形式に変換\n",
    "ds_train = Dataset.from_pandas(train_df)\n",
    "ds_test = Dataset.from_pandas(test_df) \n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"validation\": ds_test,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "- トークナイザー（文字データ⇒数値データへ変換するもの）の定義\n",
    "- BERTモデルと学習の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# トークナイザーとモデルの定義\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "ROBERTA_MODEL_NAME = \"roberta-base\"\n",
    "ALBERT_MODEL_NAME = \"albert-base-v2\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "# 学習済トークナイザーを読み込む\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n",
    "albert_tokenizer = AutoTokenizer.from_pretrained(ALBERT_MODEL_NAME)\n",
    "\n",
    "# 学習済モデルを読み込む\n",
    "bert_classification_model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME, output_attentions=True, num_labels=NUM_LABELS)\n",
    "roberta_classification_model = AutoModelForSequenceClassification.from_pretrained(ROBERTA_MODEL_NAME, output_attentions=True, num_labels=NUM_LABELS)\n",
    "albert_classification_model = AutoModelForSequenceClassification.from_pretrained(ALBERT_MODEL_NAME, output_attentions=True, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用するモデルを選択\n",
    "tokenizer = albert_tokenizer\n",
    "classification_model = albert_classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85715c7c882f4b4ab982f42779bafbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17752 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07396c25bfca40a48263da8c704be77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# トークナイザを使用してデータセットを変換する関数\n",
    "def preprocess_function(data):\n",
    "    texts = [q.strip() for q in data[\"text\"]]\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    inputs['labels'] = torch.tensor(data['label'])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# データセットを前処理変換\n",
    "tokenized_data = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 評価関数を独自に定義\n",
    "def custom_compute_metrics(res: EvalPrediction) -> Dict:\n",
    "    # res.predictions, res.label_idsはnumpyのarray\n",
    "    pred = res.predictions.argmax(axis=1)\n",
    "    target = res.label_ids\n",
    "    precision = precision_score(target, pred, average='macro')\n",
    "    recall = recall_score(target, pred, average='macro')\n",
    "    f1 = f1_score(target, pred, average='macro')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データの不均衡に対応するためにTrainerクラスを継承して独自に学習クラスを定義\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 100.0]).to(device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 学習のパラメータをセット\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./out/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 学習の設定を記述（独自学習クラス）\n",
    "trainer = CustomTrainer(\n",
    "    model=classification_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=custom_compute_metrics,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=40)],\n",
    ")\n",
    "# # 学習の設定を記述\n",
    "# trainer = Trainer(\n",
    "#     model=classification_model,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=custom_compute_metrics,\n",
    "#     train_dataset=tokenized_data[\"train\"],\n",
    "#     eval_dataset=tokenized_data[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=30)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1384' max='2219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1384/2219 04:48 < 02:54, 4.79 it/s, Epoch 0.62/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習を実行\n",
    "trainer.train(ignore_keys_for_eval=['last_hidden_state', 'hidden_states', 'attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指定したフォルダに学習モデルの保存\n",
    "trainer.save_model(\"./2308070015_roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#検証を実行\n",
    "pred_result = trainer.predict(tokenized_data[\"validation\"], ignore_keys=['loss', 'last_hidden_state', 'hidden_states', 'attentions'])\n",
    "test_df['predict'] = pred_result.predictions.argmax(axis=1).tolist()\n",
    "\n",
    "print(classification_report(test_df['label'], test_df['predict'], target_names=[\"OK\", \"NG\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 途中から学習を始める場合\n",
    "## https://github.com/huggingface/transformers/issues/7198\n",
    "# trainer.train(\"./save230806\", ignore_keys_for_eval=['last_hidden_state', 'hidden_states', 'attentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAPの算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 推論を行うためのパイプラインを設定\n",
    "sample_data = \"Possible buffer overflow to improper validation of hash segment of file while allocating memory in Snapdragon Connectivity, Snapdragon MobilePossible buffer overflow to improper validation of hash segment of file while allocating memory in Snapdragon Connectivity, Snapdragon Mobile\"\n",
    "pipe = pipeline('text-classification',model=trainer.model.cpu(), tokenizer=tokenizer)\n",
    "print(pipe(sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# パイプラインを使用してSHAPを算出\n",
    "explainer = shap.Explainer(pipe)\n",
    "shap_value = explainer([sample_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 判断根拠の出力\n",
    "shap.plots.text(shap_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
